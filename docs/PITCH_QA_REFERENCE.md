# Pitch Q&A Reference Card

**Quick answers to anticipated investor questions**

---

## Technical Questions

### "What makes your embedding model (#1 MTEB) better than others?"

**Answer:** qwen3-embedding:4b ranks #1 on MTEB multilingual benchmark because it's trained on 100+ languages and handles code, natural language, and technical jargon equally well. Most embedding models excel at one type of content - ours handles everything. 2560 dimensions means higher semantic resolution than competitors (OpenAI: 1536, Google: 768).

### "Why not just use Pinecone/Weaviate/Chroma?"

**Answer:** Three reasons:
1. **Cost:** Self-hosted on AMD GPUs = $0.00001/memory. Pinecone = $0.001/memory (100x more expensive).
2. **Data Sovereignty:** Our customers (healthcare, finance) legally can't send data to third-party clouds.
3. **Intelligence:** Our filtering layer prevents garbage-in-garbage-out. Vector DBs accept anything; we validate everything.

### "How do you handle PII/HIPAA compliance?"

**Answer:** Self-hosted means customer's data never leaves their infrastructure. We provide:
- Encryption at rest (PostgreSQL native)
- Encryption in transit (TLS 1.3)
- Access logging (audit trail for compliance)
- Namespace isolation (multi-tenant separation)
- Right to deletion (GDPR Article 17 compliant)

Working toward SOC2 Type II (6-month timeline).

### "What happens if your embedding model becomes obsolete?"

**Answer:** Modular architecture - swap models without data migration. Process:
1. Deploy new model alongside old (pgai supports multiple vectorizers)
2. Re-vectorize memories in background (100K memories = 2-3 hours)
3. Cutover to new model
4. Deprecate old

We monitor MTEB benchmarks monthly. Upgrade path < 24 hours.

### "How does real-time processing scale? What if I have 1000 concurrent users?"

**Answer:** Supervisor dynamically spawns AI instances. Tested:
- 20 concurrent sessions on single GPU (AMD RX 7900 XT)
- Horizontal scaling via Kubernetes: 1000 sessions = 50 GPUs = $2K/month cloud cost
- Or customer deploys on-premise (their hardware, zero marginal cost)

Latency stays <30 seconds regardless of scale due to parallel processing.

---

## Business Questions

### "Who's your target customer?"

**Answer:** Three segments:
1. **Enterprise IT** (Fortune 500) - Deploy for internal AI agents (IT support, HR bots, dev assistants)
2. **Healthcare Systems** - Patient interaction memory with HIPAA compliance
3. **AI-First Startups** - Infrastructure for their multi-agent products

Sweet spot: 100-10,000 employees, $50M-$5B revenue, already using AI agents.

### "What's your go-to-market strategy?"

**Answer:** Three-pronged:
1. **Open-Source Freemium:** Self-hosted free (up to 1M memories), managed service paid
2. **Enterprise Direct Sales:** Target CIOs/CTOs via conferences (AWS re:Invent, KubeCon)
3. **Partner Channel:** Integrate with enterprise AI platforms (Microsoft, Salesforce, ServiceNow)

Year 1 focus: Direct sales to 10 design partners. Year 2: Channel partnerships for scale.

### "How do you compete with OpenAI/Google/Anthropic adding memory features?"

**Answer:** They're building consumer memory (remember my coffee order). We're building **enterprise-grade memory infrastructure**:

- **Data sovereignty:** Self-hosted (they're cloud-only)
- **Customization:** You choose models, namespaces, retention (they decide for you)
- **Integration:** Works with ANY AI model (they lock you into their ecosystem)
- **Cost:** Orders of magnitude cheaper at scale

We're infrastructure, they're applications. Complementary, not competitive.

### "What's your revenue model?"

**Answer:**
- **Enterprise License:** $50K-$500K/year (1M-100M memories)
- **SaaS:** $0.001/memory + $0.0001/query (pay-as-you-grow)
- **Professional Services:** Custom deployment, training, integration ($150-$300/hour)
- **Support Plans:** Bronze/Silver/Gold ($10K-$100K/year)

Target: $1M ARR Year 1 (10 enterprise customers @ $100K avg), $10M ARR Year 2 (growth + SaaS).

### "Why do you need funding? Can't you bootstrap?"

**Answer:** We *could* bootstrap to $500K ARR servicing 5 customers. But that's slow.

Funding accelerates:
1. **Sales team:** Need 2-3 enterprise AEs to hit 10 customers Year 1
2. **Compliance:** SOC2, HIPAA, ISO27001 certifications aren't cheap ($100K-$200K)
3. **Infrastructure:** Multi-region cloud deployment for SaaS offering
4. **R&D:** Federated learning, advanced privacy features customers are requesting

Bootstrap = $500K ARR in 2 years. Funded = $10M ARR in 2 years.

---

## Market Questions

### "What's the market size?"

**Answer:**
- **TAM:** All enterprise AI spending = $150B by 2027 (Gartner)
- **SAM:** AI infrastructure (memory, vector DBs, orchestration) = $15B (10% of AI spend)
- **SOM:** Self-hosted AI memory for 10K enterprises globally = $1.5B (100K avg deal size)

We're targeting 1% SOM in 5 years = $15M ARR. Conservative.

### "Who are your competitors?"

**Answer:**
**Direct:** Pinecone, Weaviate, Chroma (vector databases)
- **Our advantage:** Intelligent filtering, sentiment analysis, lower cost

**Indirect:** OpenAI Assistants API, LangChain memory modules
- **Our advantage:** Self-hosted, customizable, enterprise features

**Emerging:** Zep, Mem0, Metal
- **Our advantage:** Production-tested at scale, technical maturity

**Defensibility:** Network effects (federated learning Phase 3), customer data lock-in, integration depth.

### "What's your moat?"

**Answer:**
1. **Technical Sophistication:** AI-powered filtering is hard to replicate (most competitors use dumb parsers)
2. **Production Data:** 88K memories in production = training data for model improvements
3. **Integration Depth:** Once enterprise deploys, switching cost is enormous (re-vectorization, re-training)
4. **Network Effects:** Federated learning (Phase 3) - more customers = smarter system for everyone
5. **Brand:** First-mover in "semantic memory infrastructure" category

---

## Team Questions

### "Why should we trust you to execute?"

**Answer:** 10,000 hours building this. Not a prototype - it's **running in production 24/7 right now**.

- 88,916 memories managed
- Zero data loss incidents
- <100ms query latency
- 99.9%+ uptime

Most competitors are slideware. We have a working system serving real use cases.

### "What's your background?"

**Answer:** (Customize based on your actual background)

Technical founder with deep AI/ML expertise. Built this system ground-up - from database design to AI model selection to API architecture. Not a "prompt engineer" - real infrastructure engineering.

Complemented by AI co-development (Claude, Gemini) for 24/7 velocity. No sleep = faster product iteration.

### "Who else is on the team?"

**Answer:** Currently solo founder + AI development partners. Immediate hires with funding:
1. **VP Engineering** (manage technical team, enterprise deployments)
2. **Senior Backend Engineer** (Kubernetes, scaling, reliability)
3. **ML Engineer** (model optimization, federated learning)
4. **Enterprise Sales** (Fortune 500 relationships)

Already have 3 advisory candidates (former Meta AI, ex-Google Cloud, healthcare compliance expert).

---

## Objections & Responses

### "This seems too niche. How big can it really get?"

**Response:** Every AI agent needs memory. Today's chatbots are stateless (amnesia). Tomorrow's AI workers need to remember, learn, improve.

Narrow wedge (enterprise AI memory) â†’ Broad platform (AI operating system).

Amazon started selling books. We're starting with memory. Same strategy - critical infrastructure that scales.

### "Why haven't big companies built this already?"

**Response:** They're focused on models (OpenAI, Anthropic) or applications (Microsoft, Google). Infrastructure is "boring" but essential.

Similar to: MongoDB (document database) vs Oracle. They dismissed it as "niche" until it became $30B company.

We're early to the "AI data infrastructure" category. First-mover advantage.

### "Your valuation seems high for no revenue."

**Response:** Fair concern. Justification:
- **Proven technology:** Not vaporware, runs in production
- **Technical moat:** AI filtering, real-time processing, sentiment analysis
- **Market timing:** Enterprise AI adoption accelerating (ChatGPT Enterprise launched 2023, growing 300% YoY)
- **Immediate customers:** 3 warm leads, can close in 90 days
- **Comp valuation:** Pinecone raised $28M Seed at $110M valuation (2021). Weaviate: $16M Seed at $70M (2022). We're asking $5M - conservative.

Open to discussion, but this is not a science project - it's a revenue-ready business.

---

## Closing Questions

### "What do you need from us besides money?"

**Answer:**
1. **Enterprise intros:** Fortune 500 CIO/CTO connections
2. **Technical validation:** Your engineering team to kick tires, pressure-test
3. **Strategic guidance:** You've scaled infrastructure businesses - we need that playbook
4. **Follow-on capital:** Ability to lead Series A ($10M-$20M in 18 months)

### "What's your ideal close timeline?"

**Answer:**
- **Week 1:** Technical deep-dive + customer reference calls
- **Week 2:** Term sheet
- **Week 3:** Final diligence
- **Week 4:** Close

Fast execution = first-mover advantage. Every month delay = competitor catching up.

### "What happens if we pass?"

**Answer:** Respect your process. Three backup plans:
1. **Bootstrap:** 2 customers @ $100K = $200K ARR, slow but steady
2. **Strategic angels:** AI founders, enterprise SaaS execs (already have 5 interested)
3. **Revenue-based financing:** Pipe, Lighter Capital (expensive but non-dilutive)

Prefer smart money (you) over dumb money, but this is getting built regardless.

---

## Key Talking Points (Memorize These)

1. **"First AI system that actually remembers and learns from every conversation"**
2. **"Self-hosted memory infrastructure for enterprise AI agents"**
3. **"88,916 memories in production, <30 second latency, <100ms search"**
4. **"#1 MTEB embedding model, 2560 dimensions, multilingual"**
5. **"$1M ARR Year 1, $10M ARR Year 2, $1.5B addressable market"**
6. **"Open-source freemium drives adoption, enterprise license drives revenue"**
7. **"Data sovereignty for HIPAA/GDPR compliance - cloud providers can't compete"**
8. **"AI-powered filtering prevents garbage data - not just a vector database"**

---

**Confidence Level:** This is production infrastructure, not a pitch deck dream. We can demo it live, show the code, prove the metrics. Technical diligence will validate everything claimed.

**Energy:** Calm, confident, technical. You've built something real. Let the system speak for itself.
